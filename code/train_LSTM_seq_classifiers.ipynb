{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for training and evaluating the LSTM classifiers\n",
    "\n",
    "This notebook reproduces results reported in Table 5 of the Living Machines paper (COLING 2020). The notebook consists of two parts, (a) training and (b) evaluation. In all experiments make use of the FLAIR framework. We use BERT as feature extractor (i.e. to obtian representation for each token) which are then forward to an LSTM Sequence Classifier that assigns classes \"O\", \"ANIMATE\" and \"INANIMATE\" to each word. In the process, we allow for fine tuning (the last three) layers of the transformer itself.\n",
    "\n",
    "### Training\n",
    "\n",
    "This section provides code for training tree models BERT-based LSTM Models.\n",
    "\n",
    "- Experiment 0 uses the Jahan et. al data.\n",
    "- Experiment 1 uses the annotations of \"machine\" sentences, based on the 19th century British Library books corpus. The target category is \"Animacy\". In this scenario, we continue fine-tuning the model trained on Jahan data in Experiment 0.\n",
    "- Experiment 2 use the same data mentioned in Experiment 1, but in this case the target category is \"Humanness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import flair\n",
    "from flair.data import Sentence\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.data import Corpus\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.embeddings import *\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from flair.datasets import DataLoader\n",
    "from flair.models import SequenceTagger\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from tools.helpers import fscore_results # write_csv_data,categorize_data,split_data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.svm import SVC,LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you use the correct FLAIR version. The cell below should return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair.__version__ == '0.6.0.post1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {'glove' : WordEmbeddings('glove'),\n",
    "                   'bert_ft' : TransformerWordEmbeddings('bert-base-uncased',\n",
    "                                                      fine_tune=True,\n",
    "                                                      layers='-1,-2,-3', \n",
    "                                                      pooling_operation='mean'), #\n",
    "    \n",
    "#                  'bert' : TransformerWordEmbeddings('bert-base-uncased',\n",
    "#                                                       fine_tune=False,\n",
    "#                                                       pooling_operation='mean'), #\n",
    "#                  'histo_bert':BertEmbeddings('/datadrive/khosseini/LM_with_bert/models/bert/FT_bert_base_uncased_after_1875_before_1890_v002/final_model')\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Experiment\n",
    "\n",
    "Select which experiment to run. Experiments are described at the beginning of this Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_dict = {0: {\"train_data\":'jahan', \"test_data\":'jahan',\"animacy\":\"animacy\"},\n",
    "                   2: {\"train_data\":'machine_sents', \"test_data\":'machine_sents',\"animacy\":\"humanness\"},\n",
    "                   1: {\"train_data\":'machine_sents', \"test_data\":'machine_sents',\"animacy\":\"animacy\"}}\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_number = 2 # 0, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Parameters\n",
    "\n",
    "Selects data based on the selected experiment. Change the `save_experiments_to` and `path_to_data` variables to the preferred folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_experiments_to = '/deezy_datadrive/kaspar-playground/rnn_experiments/coling'\n",
    "path_to_data = '/deezy_datadrive/kaspar-playground/livmach_code_review/AtypicalAnimacy/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = experiment_dict[experiment_number]['train_data']\n",
    "test_data = experiment_dict[experiment_number]['test_data']\n",
    "animacy = experiment_dict[experiment_number]['animacy']\n",
    "root_dir = Path(save_experiments_to)\n",
    "root_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = Path(path_to_data)\n",
    "\n",
    "if train_data == 'machine_sents':\n",
    "    sent_data_path_train = path_data / f'machines19thC/{animacy}_train.pkl'\n",
    "elif train_data == 'jahan':\n",
    "    sent_data_path_train = path_data / f'stories/train.pkl'\n",
    "    \n",
    "\n",
    "if test_data == 'machine_sents':\n",
    "    sent_data_path_test = path_data / f'machines19thC/{animacy}_test.pkl'\n",
    "elif test_data == 'jahan':\n",
    "    sent_data_path_test = path_data / f'stories/test.pkl'\n",
    "    \n",
    "df_train = pd.read_pickle(sent_data_path_train).sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "# further divide train into train and dev\n",
    "cutoff = int(df_train.shape[0]*.8)\n",
    "\n",
    "df_dev = df_train.iloc[cutoff:]\n",
    "df_train = df_train.iloc[:cutoff]\n",
    "\n",
    "\n",
    "df_test = pd.read_pickle(sent_data_path_test)\n",
    "print(f\"{train_data}-train\\n\",df_train.animated.value_counts())\n",
    "print(f\"{train_data}-dev\\n\",df_dev.animated.value_counts())\n",
    "print(f\"{test_data}-test\\n\",df_test.animated.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='center' # 'all' or 'center'\n",
    "masked=False # True or False\n",
    "data_format = f'sequential_{target}_{masked}'\n",
    "csv_data_path = root_dir / f'{train_data}_{animacy}_{data_format}'\n",
    "csv_data_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data in train, dev, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_data(df,path,name,target='all',masked=False,tokenize_target_expr=False,num2str = {0:'INANIMATE',1:'ANIMATE'}):\n",
    "    \n",
    "    with open(path / (name+'.txt'),'w') as out_csv:\n",
    "        \n",
    "        for i,row in df.iterrows():\n",
    "            masked_sentence = row.maskedSentence\n",
    "            if len(masked_sentence.split('[MASK]')) > 2:\n",
    "                  continue\n",
    "            if target == 'center':\n",
    "                \n",
    "                for s in masked_sentence.split('[SEP]'):\n",
    "                    if '[MASK]' in s:\n",
    "                        masked_sentence = s\n",
    "                        #print(masked_sentence)\n",
    "                        break\n",
    "            \n",
    "    \n",
    "            if masked:\n",
    "                target_expr = '[TARGET]'\n",
    "                \n",
    "            else:\n",
    "                target_expr = row.targetExpression\n",
    "            \n",
    "            try:\n",
    "                csv = []\n",
    "                masked_sentence = Sentence(masked_sentence)\n",
    "                for t in masked_sentence:\n",
    "                    #print(t.text)\n",
    "                    if not t.text == 'MASK':\n",
    "                        csv.append([t.text.lower(),'O'])\n",
    "                        #print('O')\n",
    "                    else:\n",
    "                        #print(row.Animate)\n",
    "                        csv.append(['TARGET','O'])\n",
    "                        csv.extend([[te.text.lower() ,num2str[row.animated]] for te in Sentence(target_expr)]) # row.TargetExpression\n",
    "                        csv.append(['TARGET',\"O\"])\n",
    "                \n",
    "                csv = '\\n'.join(['\\t'.join(l) for l in csv])\n",
    "                out_csv.write(f'{csv}\\n\\n')\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                #print(traceback.print_exc())\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv_data(df=df_train,\n",
    "               path=csv_data_path,\n",
    "               name='train', \n",
    "               target=target,\n",
    "               masked=masked)\n",
    "\n",
    "write_csv_data(df=df_dev,\n",
    "               path=csv_data_path,\n",
    "               name='dev',\n",
    "               target=target,\n",
    "               masked=masked)\n",
    "\n",
    "write_csv_data(df=df_test,\n",
    "               path=csv_data_path,\n",
    "               name='test',\n",
    "               target=target,\n",
    "               masked=masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 {str(csv_data_path)}/dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model hyperparameters and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings_list = ['bert_ft','glove'] # ,'glove'\n",
    "train_with_dev = False\n",
    "if train_data == 'machine_sents':\n",
    "    learning_rate = 1e-3 #.05\n",
    "    epochs = 3\n",
    "    continue_from = \"/deezy_datadrive/kaspar-playground/rnn_experiments/coling/classifier/classifier_jahan_0_seq/best-model.pt\" # path or False\n",
    "if train_data == 'jahan':\n",
    "    learning_rate = .05\n",
    "    epochs = 20\n",
    "    continue_from = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_folder = root_dir / 'classifier' \n",
    "trainer_folder.mkdir(exist_ok=True)\n",
    "trainer_path = trainer_folder/ ('classifier_' + f'{train_data}' + f'_{experiment_number}_seq')\n",
    "trainer_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save hyperparameters in file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_type = 'animacy'\n",
    "columns = {0: \"text\", 1: \"animacy\"}\n",
    "\n",
    "corpus = ColumnCorpus(csv_data_path, columns,\n",
    "                              train_file='train.txt',\n",
    "                              test_file='test.txt',\n",
    "                              dev_file='dev.txt')\n",
    "\n",
    "\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dictionary.get_items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if continue_from:\n",
    "    tagger = SequenceTagger.load(continue_from)\n",
    "    trainer = ModelTrainer(tagger, corpus)\n",
    "else:\n",
    "    embeddings = embeddings_dict['bert_ft']\n",
    "\n",
    "    tagger = SequenceTagger(hidden_size=256,\n",
    "                        embeddings=embeddings,\n",
    "                        tag_dictionary=tag_dictionary,\n",
    "                        tag_type='animacy',\n",
    "                        use_crf=False,\n",
    "                        use_rnn=True,\n",
    "                        loss_weights={'ANIMATE': 10., 'INANIMATE':10., 'O':.1}) # \n",
    "\n",
    "    trainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "results = trainer.train(trainer_path,\n",
    "              learning_rate=learning_rate,\n",
    "              mini_batch_size=32,\n",
    "              patience=3,\n",
    "              anneal_with_restarts=True,\n",
    "              monitor_test=False,\n",
    "              max_epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = '/deezy_datadrive/kaspar-playground/rnn_experiments/coling/classifier/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {model_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_path = '/deezy_datadrive/kaspar-playground/rnn_experiments/coling/classifier/classifier_machine_sents_2_seq/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {trainer_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 10 {trainer_path}\"/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level_prediction(tsv_file):\n",
    "    string2int = {'ANIMATE':1, \"INANIMATE\":0}\n",
    "    with open(tsv_file,'r') as in_tsv:\n",
    "        tsv = in_tsv.read()\n",
    "    \n",
    "    sents = tsv.strip().split(\"\\n\\n\")\n",
    "    y_true, y_pred = [], [ ]\n",
    "    for s in sents:\n",
    "        s = s.strip().split(\"\\n\")\n",
    "        y_t = [l.split(\" \")[1] for l in s if l.split(\" \")[1] != 'O']\n",
    "        y_p = [l.split(\" \")[2] for l in s if l.split(\" \")[2] != 'O']\n",
    "        \n",
    "        if y_p and y_t:\n",
    "        \n",
    "            y_true.append(string2int[max(set(y_t), key=y_t.count)])\n",
    "            y_pred.append(string2int[max(set(y_p), key=y_p.count)])\n",
    "        else:\n",
    "            print(y_t,y_p)\n",
    "    return y_true, y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred = sentence_level_prediction(Path(trainer_path) / \"test.tsv\")\n",
    "print(classification_report(y_true,y_pred,digits=3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fscore_results(y_true, y_pred,all_scores=True):\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    fscore = 0.0\n",
    "    if precision == 0 and recall == 0:\n",
    "        fscore = 0.0\n",
    "    else:\n",
    "        fscore = (2.0 * precision * recall) / (precision + recall)\n",
    "        \n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro')\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro')\n",
    "    fscore_micro = 0.0\n",
    "    if precision_micro == 0 and recall_micro == 0:\n",
    "        fscore_micro = 0.0\n",
    "    else:\n",
    "        fscore_micro = (2.0 * precision_micro * recall_micro) / (precision_micro + recall_micro)\n",
    "    \n",
    "    rank = [[y_true[x],y_pred[x]] for x in range(len(y_pred))]\n",
    "        \n",
    "    rank.sort(key=lambda x: x[1],reverse=True)\n",
    "    \n",
    "    map_ = 0\n",
    "    correct =0\n",
    "\n",
    "    for x in range(len(rank)):\n",
    "        g = rank[x][0]\n",
    "        if g== 1:\n",
    "            correct+=1\n",
    "            map_+=correct/(x+1)\n",
    "    final_map = map_/correct\n",
    "    if all_scores:\n",
    "        return round(precision,3),round(recall,3),round(fscore,3),round(final_map,3) # round(fscore_micro,3),\n",
    "    return round(fscore,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fscore_results(y_true,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
