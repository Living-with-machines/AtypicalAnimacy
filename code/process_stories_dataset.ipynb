{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process _Stories_ dataset\n",
    "\n",
    "This notebook prepares the _Stories_ dataset for the experiments in the Living Machines paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tools import animacy_evaluation,processing\n",
    "\n",
    "import spacy\n",
    "import operator\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse corpus into sentence-level dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_animacy_xml(animacy_file):\n",
    "    tree = ET.parse(animacy_file)\n",
    "    story = tree.getroot()\n",
    "    fulltext = \"\"\n",
    "    storyNumber = animacy_file.split(\"/\")[-1].split(\".sty\")[0]\n",
    "    \n",
    "    dTokens = dict()\n",
    "    dRefExp = dict()\n",
    "    dCorExp = dict()\n",
    "    \n",
    "    dAnimacyExpressions = dict()\n",
    "    processed_annotations = []\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # Capture relevant data from xml: token, refering expression, and coreferent\n",
    "    # expression annotated with animacy.\n",
    "    for section in story:\n",
    "        if section.attrib['id'] == 'edu.mit.parsing.token':\n",
    "            for child in section.findall('./desc'):\n",
    "                dTokens[child.attrib['id']] = child.text.strip()\n",
    "                \n",
    "        if section.attrib['id'] == 'edu.mit.discourse.rep.refexp':\n",
    "            for child in section.findall('./desc'):\n",
    "                reftokens = []\n",
    "                offset = int(child.attrib['off'])\n",
    "                grouped = child.text.split(\",\")\n",
    "                for gr in grouped:\n",
    "                    reftokens.append(gr.split(\"~\"))\n",
    "                dRefExp[child.attrib['id']] = (child.text, reftokens)\n",
    "                        \n",
    "        if section.attrib['id'] == 'edu.mit.discourse.rep.coref':\n",
    "            for child in section.findall('./desc'):\n",
    "                offset = int(child.attrib['off'])\n",
    "                text2mask = child.text.split(\"|\")[0]\n",
    "                coref_ani = 0\n",
    "                if 'ani' in child.attrib:\n",
    "                    if child.attrib['ani'] == '1':\n",
    "                        coref_ani = 1\n",
    "                \n",
    "                dCorExp[child.attrib['id']] = (child.text, coref_ani)\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # Capture referring expressions whose coreferent is annotated with animacy\n",
    "    for corefExp in dCorExp:\n",
    "        coref_ani = dCorExp[corefExp][1]\n",
    "        coreftext, refids = dCorExp[corefExp][0].split(\"|\")\n",
    "        refids = refids.split(\",\")\n",
    "        for refid in refids:\n",
    "            for sq in dRefExp[refid][1]:\n",
    "                dAnimacyExpressions[tuple(sq)] = ([dTokens[tk] for tk in sq], coref_ani)\n",
    "    \n",
    "    # ------------------------------------------------------\n",
    "    # Add sentence index to each token\n",
    "    fulltext = [dTokens[t] for t in dTokens]\n",
    "    \n",
    "    tokens_in_sentence = dict()\n",
    "    sIndex = 0\n",
    "    sentences = sent_tokenize(\" \".join(fulltext))\n",
    "    sentences_indices = []\n",
    "    dSentences = dict()\n",
    "    \n",
    "    for s in sentences:\n",
    "        sIndex += 1\n",
    "        sentences_indices.append((s, sIndex))\n",
    "        dSentences[sIndex] = s\n",
    "    \n",
    "    current_sentence = sentences_indices[0][0].split(\" \")\n",
    "    for t in dTokens:\n",
    "        if dTokens[t] == current_sentence[0]:\n",
    "            tokens_in_sentence[t] = (dTokens[t], sentences_indices[0][1])\n",
    "            del current_sentence[0]\n",
    "            if len(current_sentence) == 0:\n",
    "                del sentences_indices[0]\n",
    "                if sentences_indices:\n",
    "                    current_sentence = sentences_indices[0][0].split(\" \")\n",
    "        else:\n",
    "            tokens_in_sentence[t] = (dTokens[t], None)\n",
    "    \n",
    "    dSentenceIndices = dict()\n",
    "    for tk in tokens_in_sentence:\n",
    "        word_index = tk\n",
    "        sent_index = tokens_in_sentence[tk][1]\n",
    "        if sent_index in dSentenceIndices:\n",
    "            dSentenceIndices[sent_index].append(word_index)\n",
    "        else:\n",
    "            dSentenceIndices[sent_index] = [word_index]\n",
    "        \n",
    "    # ------------------------------------------------------\n",
    "    # Recover sentence where animated expression occurs, and its context\n",
    "    for expression in dAnimacyExpressions:\n",
    "        sentences_involved = list(set([tokens_in_sentence[e][1] for e in expression]))\n",
    "        sentence = 0\n",
    "        if len(sentences_involved) > 1: # Discard multisentence expressions, often annotation errors it seems\n",
    "            continue\n",
    "        else:\n",
    "            sentence = sentences_involved[0]\n",
    "        \n",
    "        token_expression = dAnimacyExpressions[expression][0]\n",
    "        animacy = dAnimacyExpressions[expression][1]\n",
    "        \n",
    "        words_sentence = dSentences[sentence].split()\n",
    "        index_sentence = dSentenceIndices[sentence]\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Find each item of the target expression in the current sentence, and mask it\n",
    "        masked_sentence = []\n",
    "        if len(words_sentence) == len(index_sentence):\n",
    "            zipped_sentence = list(zip(dSentenceIndices[sentence], dSentences[sentence].split()))\n",
    "            indexExp = 0\n",
    "            for z in zipped_sentence:\n",
    "                if indexExp < len(expression):\n",
    "                    if z[0] == expression[indexExp] and z[1] == token_expression[indexExp]:\n",
    "                        masked_sentence.append(\"[MASK]\")\n",
    "                        indexExp += 1\n",
    "                    else:\n",
    "                        masked_sentence.append(z[1])\n",
    "                else:\n",
    "                    masked_sentence.append(z[1])\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Relevant outputs:\n",
    "        prev_sentence = dSentences.get(sentence - 1, \"\")\n",
    "        current_sentence = dSentences[sentence]\n",
    "        target_expression = \" \".join(dAnimacyExpressions[expression][0]).strip()\n",
    "        masked_sentence = \" \".join(masked_sentence)\n",
    "        next_sentence = dSentences.get(sentence + 1, \"\")\n",
    "        # Replace a multi-token masked expression to just one mask, instead of\n",
    "        # having consecutive masked elements. E.g. Instead of \"[MASK] [MASK] [MASK]\n",
    "        # drank\" for \"A little man drank\", replace to \"[MASK] drank\":\n",
    "        regex_mask = r\"( ?\\[MASK\\] ?\\-?)+\" \n",
    "        masked_sentence = re.sub(regex_mask, \" [MASK] \", masked_sentence).strip()\n",
    "        \n",
    "        # ------------------------------------------------------\n",
    "        # Append relevant outputs to list to return:\n",
    "        if masked_sentence.count(\"[MASK]\") == 1: # This line is to filter out some inconsistencies in the data\n",
    "            processed_annotations.append((storyNumber, prev_sentence, current_sentence, masked_sentence, next_sentence, target_expression, animacy))\n",
    "    \n",
    "    return processed_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# Process Jahan animacy corpus, turn it to Stories corpus:\n",
    "storiesdf = pd.DataFrame()\n",
    "for i in glob.glob(\"../resources/jahan_animacy_v1.0.0/jahan_animacy_corpus_v1.0.0/data/*\"):\n",
    "    processed_annotations = parse_animacy_xml(i)\n",
    "    localdf = pd.DataFrame(processed_annotations, columns=[\"storyNumber\", \"prevSentence\", \"currentSentence\", \"maskedSentence\", \"nextSentence\", \"targetExpression\", \"animated\"])\n",
    "    storiesdf = pd.concat([storiesdf, localdf], ignore_index=True)\n",
    "    \n",
    "storiesdf[['maskedSentence', 'targetExpression']] = storiesdf.apply(lambda row: pd.Series(processing.processStories(row['targetExpression'], row['maskedSentence'], nlp)), axis=1)\n",
    "storiesdf[['context3wmasked', 'context3w']] = storiesdf.apply(lambda row: pd.Series(processing.ngram_context(row['maskedSentence'], row['targetExpression'], 3)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------\n",
    "# Split stories into training and test set, and store:\n",
    "\n",
    "trainsplit = 0.7\n",
    "\n",
    "storiesdf = storiesdf.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "train_set = storiesdf.sample(frac=trainsplit, random_state=0)\n",
    "test_set = storiesdf[~storiesdf.index.isin(train_set.index)]\n",
    "\n",
    "pathdf = \"../data/stories/\"\n",
    "Path(pathdf).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "storiesdf.to_pickle(pathdf + \"all.pkl\")\n",
    "train_set.to_pickle(pathdf + \"train.pkl\")\n",
    "test_set.to_pickle(pathdf + \"test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "storiesdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train annd test sets\")\n",
    "print(\"====================\")\n",
    "print(\"Animacy train set:\", train_set.animated.value_counts().to_dict())\n",
    "print(\"Animacy test set:\", test_set.animated.value_counts().to_dict())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Annotation counts:\n",
    "\n",
    "print(\"\\nNumber of rows\")\n",
    "print(\"==============\")\n",
    "print(\"All rows:\")\n",
    "print(\"* All:\", storiesdf.animated.count())\n",
    "print(\"* Train:\", train_set.animated.count())\n",
    "print(\"* Test:\", test_set.animated.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Load a dataset:\n",
    "df = pd.read_pickle(\"../data/stories/test.pkl\")\n",
    "\n",
    "sentInd = 21 # Select here one row in the dataframe\n",
    "print(\"***prevSentence***\", df.iloc[sentInd].prevSentence)\n",
    "print(\"***currentSentence***\", df.iloc[sentInd].currentSentence)\n",
    "print(\"***maskedSentence***\", df.iloc[sentInd].maskedSentence)\n",
    "print(\"***nextSentence***\", df.iloc[sentInd].nextSentence)\n",
    "print(\"***targetExpression***\", df.iloc[sentInd].targetExpression)\n",
    "print(\"***context3wmasked***\", df.iloc[sentInd].context3wmasked)\n",
    "print(\"***context3w***\", df.iloc[sentInd].context3w)\n",
    "print(\"***animated***\", df.iloc[sentInd].animated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent class baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_pickle(\"../data/stories/test.pkl\")\n",
    "classes_by_frequency = dataset_df.animated.value_counts(normalize=True).to_dict()\n",
    "most_frequent_class = max(classes_by_frequency.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "y_pred = [int(most_frequent_class) for x in dataset_df['animated'].tolist()]\n",
    "y_true = [int(x) for x in dataset_df['animated'].tolist()]\n",
    "\n",
    "precision, recall, fscore, micro_fscore, map_ = animacy_evaluation.results(y_true,y_pred,0.5)\n",
    "print(\"p:\", round(precision,3), \"r:\", round(recall,3), \"macro_f1:\", round(fscore,3), \"micro_f1:\", round(micro_fscore,3), \"map:\", round(map_,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37animacy)",
   "language": "python",
   "name": "py37animacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
