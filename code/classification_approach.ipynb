{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline: classifier baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from tools import animacy_evaluation,processing\n",
    "import unidecode\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import pathlib\n",
    "import pickle\n",
    "from tools import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select classifying options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select training corpus (i.e. dataset that has been used for training\n",
    "# the classifier):\n",
    "training_corpus = \"stories\" # Options: \"stories\" or \"combined\"\n",
    "\n",
    "# Select testing corpus (i.e. dataset to which classifier will be applied.\n",
    "# Its training set will be used to tune parameters, and optimal parameters\n",
    "# will be applied to its test set):\n",
    "testing_corpus = \"stories\" # Options: \"stories\" or \"machines19thC_animacy\"\n",
    "\n",
    "# Absolute path of the root directory of the github repository\n",
    "abspath = str(Path(\"../\").resolve()) + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Load validation dataset:\n",
    "dataset_validdf = pd.read_pickle(abspath + \"data/\" + testing_corpus + \"/train.pkl\")\n",
    "\n",
    "# ----------------------------------------\n",
    "# Load test dataset:\n",
    "dataset_testdf = pd.read_pickle(abspath + \"data/\" + testing_corpus + \"/test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Classify with three different classifiers:\n",
    "# * tfidf_svm: trainied in `train_svm_classifiers.ipynb`\n",
    "# * webm_svm: trainied in `train_svm_classifiers.ipynb`\n",
    "# * bert: trained in `train_bert_classifier.ipynb`\n",
    "for classifier in [\"tfidf_svm\", \"wemb_svm\", \"bert\"]:\n",
    "    \n",
    "    # Classify according to the input data:\n",
    "    # * targetExpression: the target expression itself, i.e. the expression\n",
    "    #                     we are interested in knowing its animacy.\n",
    "    # * context3wmasked: the **masked** target expression and three words\n",
    "    #                    to the left and to the right.\n",
    "    # * context3w: the target expression and three words to the left and right.\n",
    "    for type_of_training_data in [\"targetExpression\", \"context3wmasked\", \"context3w\"]:\n",
    "        \n",
    "        exp_path = abspath + \"experiments/\" + testing_corpus + \"/\"\n",
    "        Path(exp_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 1) Use validation set to find optimal animacy threshold per dataset:\n",
    "        \n",
    "        # Use classifier to return an animacy score for the sentences in the validation set:\n",
    "        y_pred = classifiers.classify(abspath,classifier,dataset_validdf[type_of_training_data],type_of_training_data,training_corpus)\n",
    "        y_true = [x for x in dataset_validdf['animated'].tolist()]\n",
    "        \n",
    "        # Each result-per-threshold is stored in the `df_results` dataframe:\n",
    "        df_results = pd.DataFrame(columns = ['threshold', 'precision', 'recall', 'fscore', 'micro_fscore', 'map'])\n",
    "        \n",
    "        # Find the optimal threshold for animacy scores from the validation set, and store in `df_results`:\n",
    "        threshold_list = list(np.arange(0, 1.05, 0.1))\n",
    "        for th in threshold_list:\n",
    "            th = float(round(th,2))\n",
    "            precision, recall, fscore, micro_fscore,map_ = animacy_evaluation.results(y_true,y_pred,th)\n",
    "            df_results = df_results.append({'threshold':th, 'precision':round(precision,3), 'recall':round(recall,3), 'fscore':round(fscore,3), 'micro_fscore':round(micro_fscore,3), 'map':round(map_,3)}, ignore_index=True)\n",
    "\n",
    "        # Sort validation set results-per-threshold according to highest f1-score, and store:\n",
    "        df_results.sort_values(by='fscore', ascending=False).to_csv(exp_path + \"classifier_\" + training_corpus + \"_\" + classifier + \"_\" + type_of_training_data + \".tsv\", sep=\"\\t\")\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # 2) Apply classifier and best animacy threhold to test set:\n",
    "        \n",
    "        # Read stored best parameters:\n",
    "        parameters_best = pd.read_csv(exp_path + \"classifier_\" + training_corpus + \"_\" + classifier + \"_\" + type_of_training_data + \".tsv\", sep=\"\\t\").iloc[0]\n",
    "        obs_threshold = parameters_best['threshold']\n",
    "\n",
    "        # Apply classifier:\n",
    "        y_pred = classifiers.classify(abspath,classifier,dataset_testdf[type_of_training_data],type_of_training_data,training_corpus)\n",
    "        y_true = [x for x in dataset_testdf['animated'].tolist()]\n",
    "\n",
    "        # Evaluate:\n",
    "        precision, recall, fscore, micro_fscore,map_ = animacy_evaluation.results(y_true,y_pred,obs_threshold)\n",
    "        \n",
    "        # Print scenario:\n",
    "        print(\"\\nType of training data:\", type_of_training_data)\n",
    "        print(\"Classifier:\", classifier)\n",
    "        print(\"Corpus used to train the classifier:\", training_corpus)\n",
    "        print(\"Corpus used for parameter tuning and testing:\", testing_corpus)\n",
    "        print(\"Results:\")\n",
    "        print(type_of_training_data, classifier, \"(t=\" + str(round(obs_threshold,2)) + \") & \" + str(round(precision,3)) + \" & \" + str(round(recall,3)) + \" & \" + str(round(fscore,3)) + \" & \" + str(round(map_,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py37animacy)",
   "language": "python",
   "name": "py37animacy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
