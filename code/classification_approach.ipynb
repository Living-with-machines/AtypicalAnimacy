{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline: classifier baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from tools import animacy_evaluation,processing\n",
    "import unidecode\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import pathlib\n",
    "import pickle\n",
    "from tools import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Select corpus to classify:\n",
    "classifier_corpus = \"stories/\" # Options: \"stories/\" or \"machines19thC\n",
    "testing_corpus = \"machines19thC/\" # Options: \"stories/\" or \"machines19thC\"\n",
    "\n",
    "abspath = \"/Users/mcollardanuy/Documents/githubCode/AtypicalAnimacy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Load data\n",
    "scenarios = {\"stories/\" : \"all_\", \"machines19thC/\": \"animacy_all_\"}\n",
    "classifier_scenario = scenarios[classifier_corpus]\n",
    "testing_scenario = scenarios[testing_corpus]\n",
    "\n",
    "dataset_traindf = pd.read_pickle(abspath + \"data/\" + testing_corpus + testing_scenario + \"train.pkl\")\n",
    "dataset_traindf['both_masked'] = dataset_traindf.apply(lambda row: processing.determine_context(\"maskedSentence\", row, \"both\"), axis=1)\n",
    "dataset_traindf['both_unmasked'] = dataset_traindf.apply(lambda row: processing.determine_context(\"currentSentence\", row, \"both\"), axis=1)\n",
    "\n",
    "dataset_testdf = pd.read_pickle(abspath + \"data/\" + testing_corpus + testing_scenario + \"test.pkl\")\n",
    "dataset_testdf['both_masked'] = dataset_testdf.apply(lambda row: processing.determine_context(\"maskedSentence\", row, \"both\"), axis=1)\n",
    "dataset_testdf['both_unmasked'] = dataset_testdf.apply(lambda row: processing.determine_context(\"currentSentence\", row, \"both\"), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in [\"tfidf_svm\", \"wemb_svm\"]:\n",
    "    for type_of_training_data in [\"targetExpression\", \"context3wmasked\", \"context3w\", \"both_masked\", \"both_unmasked\", \"currentSentence\", \"maskedSentence\"]:\n",
    "\n",
    "        threshold_list = list(np.arange(0, 1.05, 0.1))\n",
    "\n",
    "        df_results = pd.DataFrame(columns = ['threshold', 'precision', 'recall', 'fscore', 'micro_fscore', 'map'])\n",
    "        print(\"\\nType of training data:\", type_of_training_data)\n",
    "        print(\"Classifier:\", classifier)\n",
    "        print(\"Corpus used to train the classifier:\", classifier_corpus)\n",
    "        print(\"Corpus used for parameter-tuning and testing:\", testing_corpus)\n",
    "\n",
    "        if not Path(abspath + \"experiments/\" + testing_corpus + \"classifier_\" + testing_scenario + \"train_\" + classifier + \"_\" + type_of_training_data + \".tsv\").exists():\n",
    "\n",
    "            # -------------------------------------------------\n",
    "            # Optimal threshold from train:\n",
    "            for th in threshold_list:\n",
    "                print(\"* Tuning parameters.\")\n",
    "                th = float(round(th,2))\n",
    "                y_pred = classifiers.classify(abspath,classifier,dataset_traindf[type_of_training_data],type_of_training_data,classifier_corpus,classifier_scenario)\n",
    "                y_true = [x for x in dataset_traindf['animated'].tolist()]\n",
    "\n",
    "                precision, recall, fscore, micro_fscore,map_ = animacy_evaluation.results(y_true,y_pred,th)\n",
    "                df_results = df_results.append({'threshold':th, 'precision':round(precision,3), 'recall':round(recall,3), 'fscore':round(fscore,3), 'micro_fscore':round(micro_fscore,3), 'map':round(map_,3)}, ignore_index=True)\n",
    "\n",
    "            df_results.sort_values(by='fscore', ascending=False).to_csv(abspath + \"experiments/\" + testing_corpus + \"classifier_\" + testing_scenario + \"train_\" + classifier + \"_\" + type_of_training_data + \".tsv\", sep=\"\\t\")\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Apply to test:\n",
    "        \n",
    "        # Read stored best parameters:\n",
    "        parameters_best = pd.read_csv(abspath + \"experiments/\" + testing_corpus + \"classifier_\" + testing_scenario + \"train_\" + classifier + \"_\" + type_of_training_data + \".tsv\", sep=\"\\t\").iloc[0]\n",
    "        obs_threshold = parameters_best['threshold']\n",
    "\n",
    "        # Apply classifier:\n",
    "        y_pred = classifiers.classify(abspath,classifier,dataset_testdf[type_of_training_data],type_of_training_data,classifier_corpus,classifier_scenario)\n",
    "        y_true = [x for x in dataset_testdf['animated'].tolist()]\n",
    "\n",
    "        # Evaluate:\n",
    "        precision, recall, fscore, micro_fscore,map_ = animacy_evaluation.results(y_true,y_pred,obs_threshold)\n",
    "        print(\"Results:\")\n",
    "        print(type_of_training_data, classifier, \"(t=\" + str(round(obs_threshold,2)) + \") & \" + str(round(precision,3)) + \" & \" + str(round(recall,3)) + \" & \" + str(round(fscore,3)) + \" & \" + str(round(map_,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lwmbert] *",
   "language": "python",
   "name": "conda-env-lwmbert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
