{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation pipeline: classifier baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer,BertForMaskedLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import operator\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models import Word2Vec\n",
    "from tools import animacy_evaluation,processing\n",
    "import unidecode\n",
    "from collections import Counter\n",
    "import sklearn\n",
    "import pathlib\n",
    "import pickle\n",
    "from tools import processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select classifying options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select if you want to reuse stored files from previous runs:\n",
    "overwrite = True # Options: True or False (boolean)\n",
    "\n",
    "# Select training corpus (i.e. dataset that has been used for training\n",
    "# the classifier):\n",
    "training_corpus = \"stories\" # Options: \"stories\" or \"combined\"\n",
    "\n",
    "# Select testing corpus (i.e. dataset to which classifier will be applied.\n",
    "# Its training set will be used to tune parameters, and optimal parameters\n",
    "# will be applied to its test set):\n",
    "testing_corpus = \"machines19thC\" # Options: \"stories\" or \"machines19thC\"\n",
    "\n",
    "# Absolute path of the root directory of the AtypicalAnimacy github repository\n",
    "abspath = \"/Users/mcollardanuy/Documents/githubCode/AtypicalAnimacy/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------\n",
    "# Load data:\n",
    "dataset_validdf = pd.read_pickle(abspath + \"data/\" + testing_corpus + \"/train.pkl\")\n",
    "dataset_validdf['both_masked'] = dataset_validdf.apply(lambda row: processing.determine_context(\"maskedSentence\", row, \"both\"), axis=1)\n",
    "dataset_validdf['both_unmasked'] = dataset_validdf.apply(lambda row: processing.determine_context(\"currentSentence\", row, \"both\"), axis=1)\n",
    "\n",
    "dataset_testdf = pd.read_pickle(abspath + \"data/\" + testing_corpus + \"/test.pkl\")\n",
    "dataset_testdf['both_masked'] = dataset_testdf.apply(lambda row: processing.determine_context(\"maskedSentence\", row, \"both\"), axis=1)\n",
    "dataset_testdf['both_unmasked'] = dataset_testdf.apply(lambda row: processing.determine_context(\"currentSentence\", row, \"both\"), axis=1)\n",
    "\n",
    "# ----------------------------------------\n",
    "# Classify:\n",
    "for classifier in [\"tfidf_svm\", \"wemb_svm\", \"bert\"]:\n",
    "    for type_of_training_data in [\"targetExpression\", \"context3wmasked\", \"context3w\"]:\n",
    "\n",
    "        threshold_list = list(np.arange(0, 1.05, 0.1))\n",
    "\n",
    "        df_results = pd.DataFrame(columns = ['threshold', 'precision', 'recall', 'fscore', 'micro_fscore', 'map'])\n",
    "        \n",
    "        print(\"\\nType of training data:\", type_of_training_data)\n",
    "        print(\"Classifier:\", classifier)\n",
    "        print(\"Corpus used to train the classifier:\", training_corpus)\n",
    "        print(\"Corpus used for parameter tuning and testing:\", testing_corpus)\n",
    "\n",
    "        if not Path(abspath + \"experiments/\" + testing_corpus + \"/classifier_\" + training_corpus + \"_\" + classifier + \"_\" + type_of_training_data + \".tsv\").exists() or overwrite == True:\n",
    "\n",
    "            # -----------------------------------------------\n",
    "            # Optimal threshold from the validation dataframe (dataset_validdf) :\n",
    "            for th in threshold_list:\n",
    "                th = float(round(th,2))\n",
    "                print(\"* Tuning parameters,\", th)\n",
    "                y_pred = classifiers.classify(abspath,classifier,dataset_validdf[type_of_training_data],type_of_training_data,training_corpus)\n",
    "                y_true = [x for x in dataset_validdf['animated'].tolist()]\n",
    "\n",
    "                precision, recall, fscore, micro_fscore,map_ = animacy_evaluation.results(y_true,y_pred,th)\n",
    "                df_results = df_results.append({'threshold':th, 'precision':round(precision,3), 'recall':round(recall,3), 'fscore':round(fscore,3), 'micro_fscore':round(micro_fscore,3), 'map':round(map_,3)}, ignore_index=True)\n",
    "\n",
    "            df_results.sort_values(by='fscore', ascending=False).to_csv(abspath + \"experiments/\" + testing_corpus + \"/classifier_\" + training_corpus + \"_\" + classifier + \"_\" + type_of_training_data + \".tsv\", sep=\"\\t\")\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Apply to test:\n",
    "        \n",
    "        # Read stored best parameters:\n",
    "        parameters_best = pd.read_csv(abspath + \"experiments/\" + testing_corpus + \"/classifier_\" + training_corpus + \"_\" + classifier + \"_\" + type_of_training_data + \".tsv\", sep=\"\\t\").iloc[0]\n",
    "        obs_threshold = parameters_best['threshold']\n",
    "\n",
    "        # Apply classifier:\n",
    "        y_pred = classifiers.classify(abspath,classifier,dataset_testdf[type_of_training_data],type_of_training_data,training_corpus)\n",
    "        y_true = [x for x in dataset_testdf['animated'].tolist()]\n",
    "\n",
    "        # Evaluate:\n",
    "        precision, recall, fscore, micro_fscore,map_ = animacy_evaluation.results(y_true,y_pred,obs_threshold)\n",
    "        print(\"Results:\")\n",
    "        print(type_of_training_data, classifier, \"(t=\" + str(round(obs_threshold,2)) + \") & \" + str(round(precision,3)) + \" & \" + str(round(recall,3)) + \" & \" + str(round(fscore,3)) + \" & \" + str(round(map_,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lwmbert] *",
   "language": "python",
   "name": "conda-env-lwmbert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
