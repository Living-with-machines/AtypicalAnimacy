{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process _machines19thC_ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tools import animacy_evaluation,processing\n",
    "import operator\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_all = pd.read_csv(\"/Users/mcollardanuy/Downloads/GT_MCA_annotations_DSH - Sheet1.tsv\", sep=\"\\t\")\n",
    "master_all = master_all.drop(\"Unnamed: 0\", axis=1)\n",
    "master_all = master_all.drop_duplicates(subset=[\"SentenceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_all_dw = pd.read_csv(\"/Users/mcollardanuy/Downloads/DW_annotations_DSH - Sheet1.tsv\", sep=\"\\t\")\n",
    "master_all_dw = master_all_dw.drop(\"Unnamed: 0\", axis=1)\n",
    "master_all_dw = master_all_dw.drop_duplicates(subset=[\"SentenceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dAnimacyGT = dict()\n",
    "dHumannessGT = dict()\n",
    "dAnimacyMCA = dict()\n",
    "dHumannessMCA = dict()\n",
    "for i, row in master_all.iterrows():\n",
    "    dAnimacyGT[row[\"SentenceId\"]] = row[\"Animacy_GT\"]\n",
    "    dHumannessGT[row[\"SentenceId\"]] = row[\"Humanness_GT\"]\n",
    "    dAnimacyMCA[row[\"SentenceId\"]] = row[\"Animacy_MCA\"]\n",
    "    dHumannessMCA[row[\"SentenceId\"]] = row[\"Humanness_MCA\"]\n",
    "    \n",
    "dAnimacyDW = dict()\n",
    "dHumannessDW = dict()\n",
    "for i, row in master_all_dw.iterrows():\n",
    "    dAnimacyDW[row[\"SentenceId\"]] = row[\"Animacy_DW\"]\n",
    "    dHumannessDW[row[\"SentenceId\"]] = row[\"Humanness_DW\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "majority_vote_animacy = dict()\n",
    "for s in dAnimacyGT:\n",
    "    if s in dAnimacyGT and s in dAnimacyMCA and s in dAnimacyDW:\n",
    "        if not np.isnan(dAnimacyGT[s]) and not np.isnan(dAnimacyMCA[s]) and not np.isnan(dAnimacyDW[s]):\n",
    "            animacy_annotations = [int(dAnimacyGT[s]), int(dAnimacyMCA[s]), int(dAnimacyDW[s])]\n",
    "            majority_vote_animacy[s] = max(set(animacy_annotations), key = animacy_annotations.count)\n",
    "        else:\n",
    "            majority_vote_animacy[s] = None\n",
    "    else:\n",
    "        majority_vote_animacy[s] = None\n",
    "            \n",
    "majority_vote_humanness = dict()\n",
    "for s in dHumannessGT:\n",
    "    if s in dHumannessGT and s in dHumannessMCA and s in dHumannessDW:\n",
    "        if not np.isnan(dHumannessGT[s]) and not np.isnan(dHumannessMCA[s]) and not np.isnan(dHumannessDW[s]):\n",
    "            humanness_annotations = [int(dHumannessGT[s]), int(dHumannessMCA[s]), int(dHumannessDW[s])]\n",
    "            majority_vote_humanness[s] = max(set(humanness_annotations), key = humanness_annotations.count)\n",
    "        else:\n",
    "            majority_vote_humanness[s] = None\n",
    "    else:\n",
    "        majority_vote_humanness[s] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []\n",
    "prevSentence = []\n",
    "currentSentence = []\n",
    "maskedSentence = []\n",
    "nextSentence = []\n",
    "targetExpression = []\n",
    "animated = []\n",
    "human = []\n",
    "context3wmasked = []\n",
    "context3w = []\n",
    "\n",
    "master_all[\"animacy_majority\"] = master_all['SentenceId'].map(majority_vote_animacy)\n",
    "master_all[\"humanness_majority\"] = master_all['SentenceId'].map(majority_vote_humanness)\n",
    "\n",
    "master_all[[\"prevSentence\", \"currentSentence\", \"maskedSentence\", \"nextSentence\"]] = master_all.apply(lambda row: pd.Series(processing.processMachines19thC(row['Sentence'], row['SentenceCtxt'], row['TargetExpression'], nlp)), axis=1)\n",
    "master_all = master_all.rename(columns={\"TargetExpression\": \"targetExpression\"})\n",
    "master_all = master_all[master_all[\"maskedSentence\"].str.contains(\"[MASK]\", regex=False)]\n",
    "master_all[['context3wmasked', 'context3w']] = master_all.apply(lambda row: pd.Series(processing.ngram_context(row['maskedSentence'], row['targetExpression'], 3)), axis=1)\n",
    "\n",
    "animacy_all = pd.DataFrame()\n",
    "animacy_all[\"date\"] = master_all[\"Date\"]\n",
    "animacy_all[\"SentenceId\"] = master_all[\"SentenceId\"]\n",
    "animacy_all[\"prevSentence\"] = master_all[\"prevSentence\"]\n",
    "animacy_all[\"currentSentence\"] = master_all[\"currentSentence\"]\n",
    "animacy_all[\"maskedSentence\"] = master_all[\"maskedSentence\"]\n",
    "animacy_all[\"nextSentence\"] = master_all[\"nextSentence\"]\n",
    "animacy_all[\"targetExpression\"] = master_all[\"targetExpression\"]\n",
    "animacy_all[\"context3wmasked\"] = master_all[\"context3wmasked\"]\n",
    "animacy_all[\"context3w\"] = master_all[\"context3w\"]\n",
    "animacy_all[\"animated\"] = master_all[\"animacy_majority\"]\n",
    "animacy_all = animacy_all[animacy_all[\"animated\"].notnull()]\n",
    "animacy_all = animacy_all.reset_index()\n",
    "animacy_all[\"animated\"] = animacy_all[\"animated\"].astype('int64')\n",
    "\n",
    "humanness_all = pd.DataFrame()\n",
    "humanness_all[\"date\"] = master_all[\"Date\"]\n",
    "humanness_all[\"SentenceId\"] = master_all[\"SentenceId\"]\n",
    "humanness_all[\"prevSentence\"] = master_all[\"prevSentence\"]\n",
    "humanness_all[\"currentSentence\"] = master_all[\"currentSentence\"]\n",
    "humanness_all[\"maskedSentence\"] = master_all[\"maskedSentence\"]\n",
    "humanness_all[\"nextSentence\"] = master_all[\"nextSentence\"]\n",
    "humanness_all[\"targetExpression\"] = master_all[\"targetExpression\"]\n",
    "humanness_all[\"context3wmasked\"] = master_all[\"context3wmasked\"]\n",
    "humanness_all[\"context3w\"] = master_all[\"context3w\"]\n",
    "humanness_all[\"animated\"] = master_all[\"humanness_majority\"]\n",
    "humanness_all = humanness_all[humanness_all[\"animated\"].notnull()]\n",
    "humanness_all = humanness_all.reset_index()\n",
    "humanness_all[\"animated\"] = humanness_all[\"animated\"].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathdf = \"../data/machines19thC/\"\n",
    "Path(pathdf).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#-----------------------------------------------------\n",
    "# Store machines19thC animacy dataframe:\n",
    "animacy_all = animacy_all.drop(\"index\", axis=1)\n",
    "animacy_all.to_pickle(pathdf + \"animacy.pkl\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Store machines19thC humanness dataframe:\n",
    "humanness_all = humanness_all.drop(\"index\", axis=1)\n",
    "humanness_all.to_pickle(pathdf + \"humanness.pkl\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Split into train and test set and store:\n",
    "\n",
    "trainsplit = 0.3\n",
    "\n",
    "animacy_all = animacy_all.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "animacy_all_train_set = animacy_all.sample(frac=trainsplit)\n",
    "animacy_all_test_set = animacy_all[~animacy_all.index.isin(animacy_all_train_set.index)]\n",
    "\n",
    "animacy_all_train_set.to_pickle(pathdf + \"train.pkl\")\n",
    "animacy_all_test_set.to_pickle(pathdf + \"test.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train annd test sets\n",
      "====================\n",
      "Animacy train set: {0: 120, 1: 58}\n",
      "Animacy test set: {0: 272, 1: 143}\n",
      "\n",
      "Number of rows\n",
      "==============\n",
      "All rows:\n",
      "* All: 593\n",
      "* Train: 178\n",
      "* Test: 415\n"
     ]
    }
   ],
   "source": [
    "print(\"Train annd test sets\")\n",
    "print(\"====================\")\n",
    "print(\"Animacy train set:\", animacy_all_train_set.animated.value_counts().to_dict())\n",
    "print(\"Animacy test set:\", animacy_all_test_set.animated.value_counts().to_dict())\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Annotation counts:\n",
    "\n",
    "print(\"\\nNumber of rows\")\n",
    "print(\"==============\")\n",
    "print(\"All rows:\")\n",
    "print(\"* All:\", animacy_all.animated.count())\n",
    "print(\"* Train:\", animacy_all_train_set.animated.count())\n",
    "print(\"* Test:\", animacy_all_test_set.animated.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***prevSentence*** These machines are chain-boats.\n",
      "***currentSentence*** Instead of having either screw or paddle wheels, both of which would, by their wash, damage the banks of the canal, the engine turns a large \" sprocket\" wheel amidships.\n",
      "***maskedSentence*** Instead of having either screw or paddle wheels, both of which would, by their wash, damage the banks of the canal, the [MASK] turns a large \" sprocket\" wheel amidships.\n",
      "***nextSentence*** A chain, which is laid the whole length of the canal, is brought to this wheel, and the tug winds herself along, much after the fashion of the floating bridge at Portsmouth.\n",
      "***targetExpression*** engine\n",
      "***context3wmasked*** the canal, the [MASK] turns a large\n",
      "***context3w*** the canal, the engine turns a large\n",
      "***animated*** 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle(pathdf + \"train.pkl\")\n",
    "\n",
    "sentInd = 1\n",
    "print(\"***prevSentence***\", df.iloc[sentInd].prevSentence)\n",
    "print(\"***currentSentence***\", df.iloc[sentInd].currentSentence)\n",
    "print(\"***maskedSentence***\", df.iloc[sentInd].maskedSentence)\n",
    "print(\"***nextSentence***\", df.iloc[sentInd].nextSentence)\n",
    "print(\"***targetExpression***\", df.iloc[sentInd].targetExpression)\n",
    "print(\"***context3wmasked***\", df.iloc[sentInd].context3wmasked)\n",
    "print(\"***context3w***\", df.iloc[sentInd].context3w)\n",
    "print(\"***animated***\", df.iloc[sentInd].animated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent class baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: 0.328 r: 0.5 macro_f1: 0.396 micro_f1: 0.655 map: 0.316\n"
     ]
    }
   ],
   "source": [
    "dataset_df = pd.read_pickle(pathdf + \"test.pkl\")\n",
    "classes_by_frequency = dataset_df.animated.value_counts(normalize=True).to_dict()\n",
    "most_frequent_class = max(classes_by_frequency.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "y_pred = [int(most_frequent_class) for x in dataset_df['animated'].tolist()]\n",
    "y_true = [int(x) for x in dataset_df['animated'].tolist()]\n",
    "\n",
    "precision, recall, fscore, micro_fscore, map_ = animacy_evaluation.results(y_true,y_pred,0.5)\n",
    "print(\"p:\", round(precision,3), \"r:\", round(recall,3), \"macro_f1:\", round(fscore,3), \"micro_f1:\", round(micro_fscore,3), \"map:\", round(map_,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create combined training set (`stories` and `machines19thC`)\n",
    "\n",
    "Make sure you have run `process_stories_dataset.ipynb` before you run the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpstories = pd.read_pickle(\"../data/stories/train.pkl\")\n",
    "tmpstories = tmpstories.drop(\"storyNumber\", axis=1)\n",
    "tmpmachines = pd.read_pickle(\"../data/machines19thC/train.pkl\")\n",
    "tmpmachines = tmpmachines.drop([\"date\", \"SentenceId\"], axis=1)\n",
    "combineddf = pd.concat([tmpstories, tmpmachines], ignore_index=True, sort=True)\n",
    "combineddf = combineddf.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "pathcomb = \"../data/combined/\"\n",
    "Path(pathcomb).mkdir(parents=True, exist_ok=True)\n",
    "combineddf.to_pickle(pathcomb + \"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lwmbert)",
   "language": "python",
   "name": "lwmbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
